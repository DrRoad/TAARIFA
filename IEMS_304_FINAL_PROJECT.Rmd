---
title: "IEMS 304 Final Project"
author: "Eric Chang, Chen Lin, Haoyu Wang, Allie Woodhouse"
date: "Tuesday, March 01, 2016"
output: html_document
---
```{r}
# We will be loading the finished models from a workspace to avoid having to run the computationally intensive machine learning models every time.
load("FinalProject.RData")
```

Bagging, boosted tree, random forest


## Data prep and variable selection

Notes:

1. Combined "non functional" and "functional needs repair" response variable.
2. Filtered redundant variables
3. Added `construction year` and `population` missing value dummy

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)

# DATAPREP Function
dataprep <- function(train){
train <- select(train, status_group, gps_height, date_recorded,
                   amount_tsh, longitude, latitude,
                   basin, region, 
                   population, public_meeting, 
                   scheme_management, permit, construction_year,
                   extraction_type_class,
                   management_group, payment, payment_type,
                   water_quality, quantity,
                   source_type, waterpoint_type)

# Construction year variable - dealing with missing values
train$construction_year_miss = ifelse(train$construction_year==0, 1, 0)
train$construction_year_miss <- as.factor(train$construction_year_miss)

# Construction year variable - dealing with missing values
train$population_miss = ifelse(train$population==0, 1, 0)
train$population_miss <- as.factor(train$population_miss)

return(train)
}

# Read data
df.labels <- read.csv("train_labels.csv")
df.values  <- read.csv("train_values.csv")

# Join dataset and call prep function
train <- left_join(df.values, df.labels, by="id")
train <- dataprep(train)

# Reduce 3 level categorical response var to binomial 
train$status_group <- as.character(train$status_group)

train <- train %>%
  mutate(status_group = replace(status_group, status_group=="functional", 1),
         status_group = replace(status_group, status_group=="non functional", 0),
         status_group = replace(status_group, status_group=="functional needs repair", 0))
train$status_group <- as.factor(train$status_group)

```


## Exploratory Analysis

```{r, eval=TRUE}
# Longitude and Latitude
train <- filter(train, longitude > 1)

ggplot(data=train) + 
  geom_point(aes(x=longitude, y=latitude, 
                  color=status_group, alpha=0.2, size=.5), size=.5)
```
![](LongLat.png)

The clustering of the good pumps as well as the nonlinearity of the clustering among the x and y (longitude and latitude) axes lead us to believe that we should include an interaction and spline in our model.  

The geographical nature of our data makes the interactions among our variables potentially very complicated and as such we will compare the effectiveness of a machine learning model to a regression model.

### A Geographical Model

```{r, message=FALSE, warning=FALSE}
# FUNCTION eval.glm: returns classification rate and AUC of glm on test set data
eval.glm <- function(glm, data=test.glm){
  # Calculate classification rate
  data$pred <- predict(glm, newdata=data, type="response")
  data$pred <- ifelse(data$pred > .5, 1, 0)
  data$success <- ifelse(data$pred == data$status_group, 1, 0)
  classrate <- prop.table(table(data$success))
  
  # Calculate AUC
  auc <- auc(data$status_group, predict(glm, newdata=data, type="response"))

  cat("Classification Rate:", as.character(classrate[[2]]), "\nAUC:", auc)
}
```

```{r, message=FALSE, warning=FALSE}
library(splines); library(pROC);

# Split into 80/20 Train/Test Set
train$status_group <- as.factor(train$status_group)
train.glm <- sample_frac(train, .8)
test.glm <- setdiff(train, train.glm)

# Treating longitude and latitude as linear
lin <- glm(status_group ~ longitude + latitude, data=train.glm, family=binomial)
eval.glm(lin)

# Treating longitude and latitude as splines
spl <- glm(status_group ~ bs(longitude) + bs(latitude), data=train.glm, family=binomial)
eval.glm(spl)
```

The out of sample estimate of AUC is dismal on both accounts - worse than the null of 0.50 on the linear model and only slightly better when treating longtitude and latitude as splines. We now try adding multiple fold interactions.

```{r, message=FALSE, warning=FALSE}
# 2 way interactions
glm2 <- glm(status_group ~ (bs(longitude) + bs(latitude))^2, 
             data=train.glm, family=binomial)
eval.glm(glm2)

# 3 way interactions
glm3 <- glm(status_group ~ (bs(longitude) + bs(latitude))^3, 
             data=train.glm, family=binomial)
eval.glm(glm3)

# 4 way interactions
glm4 <- glm(status_group ~ (bs(longitude) + bs(latitude))^4, 
             data=train.glm, family=binomial)
eval.glm(glm4)
```

Adding interactions only improves our AUC up until the 2-fold level. Now we will try a random forest.

```{r, message=FALSE, eval=FALSE}
library(randomForest)
rf1  <-  randomForest(as.factor(status_group) ~ longitude + latitude, data=train, ntree=500)
```
```{r}
rf1
```


The random forest reflects a drastic improvement over the regression models, with a correct classification rate of . Our insight from this result is that because we are using the same two variables, `longitude` and `latitude`, there are complex interactions and transformations in the data that are not captured by a simple regression model, and in this case, machine learning techniques will likely prove to be more effective for categorization. This idea is supported by the complex map we obtained of the clustering of pumps.


## Full Machine Learning Models

Because we are now exclusively using machine learning techniques to categorize the Tanzanian water pumps, we will return to the original 3-level categorical response variable as the reason we reduced it earlier was to avoid the use of multinomial logistic regression.
```{r}
# Return to 3 level categorical response variable
train <- left_join(df.values, df.labels, by="id")
train <- select(dataprep(train), -date_recorded)
```

### Random Forests and Bagging
```{r, eval=FALSE}
# 500 tree random forest
rf.500 <- randomForest(as.factor(status_group) ~ ., data=train)
```
```{r}
rf.500
```
With the full model in a 500 tree random forest, we obtain an out-of-bag classification rate of .8097 Let's try doubling the number of trees.

```{r, eval=FALSE}
# 1000 tree random forest
rf.1000 <- randomForest(as.factor(status_group) ~ ., data=train, ntree=1000)
```
```{r}
rf.1000
```
Increasing the number of trees grown results in an out-of-bag classification rate of .8112, improving our classification rate slightly. 


```{r, eval=FALSE}
# Bagging
bagging <- randomForest(as.factor(status_group) ~ ., data=train, mtry=(ncol(train)-1))
```
```{r}
bagging
```
The bagging algorithm used with 500 trees gives a classification rate of .8055, worse than both random forests using $\sqrt{p}$ predictors, verifying that the instability introduced by reducing the number of predictors actually increases the predictive accuracy of random forests.

### Boosted Trees
```{r}
library(gbm)
# Create training set and test set
train.gbm <- sample_frac(train, .8)
test.gbm <- setdiff(train, train.gbm)

# FUNCTION eval.gbm: evaluates boosted tree on test set and returns correct classification rate.
eval.gbm <- function(gbm, data=test.gbm, n.trees){
  library(reshape2); library(dplyr); library(magrittr);
  
  probs <- predict(gbm, data, n.trees=n.trees, type="response") #generate gbm predictions on test set
  out <- data.frame(data$status_group, probs)
  
  names(out)[2:4] <- c("functional", "functional needs repair", "non functional")
  out$x <- row.names(out) # Name all rows
  
  # Find the maximum probability response for each row
  response <- 
    out %>%
    melt() %>% # Melt on status_group variable
    group_by(x) %>%
    summarise(max(value)) %>%
    arrange(as.numeric(x))
  
  out <- inner_join(melt(out), response, by=c("x", "value"="max(value)"))
  out$success <- ifelse(out[,1]==out[,3], 1, 0)
  
  classrate <- sum(out$success)/nrow(out)

  cat("Classification Rate:", as.character(classrate))
}
```

```{r, eval=FALSE}
# Default boosted tree
gbm1 <- gbm(status_group ~ ., data=train.gbm, distribution="multinomial")
```
```{r, message=FALSE}
eval.gbm(gbm1, n.trees=100)
```

```{r, eval=FALSE}
# Default boosted tree
# Increasing number of trees
gbm2 <- gbm(status_group ~ ., data=train.gbm, distribution="multinomial", n.trees=1000)
```
```{r, message=FALSE}
eval.gbm(gbm2, n.trees=1000)
```

```{r, eval=FALSE}
# Changing depth and learning rate
gbm3 <- gbm(status_group ~ ., data=train.gbm, distribution="multinomial", 
           n.trees=1000, interaction.depth = 2, shrinkage = .01)
```
```{r, message=FALSE}
eval.gbm(gbm3, n.trees=1000)
```

```{r, eval=FALSE}
# Increaing depth again, increase number of trees
gbm4 <- gbm(status_group ~ ., data=train.gbm, distribution="multinomial", 
           n.trees=1000, interaction.depth = 3, shrinkage = .01)
```
```{r, message=FALSE}
eval.gbm(gbm4, n.trees=1000)
```

## Improving our model through feature engineering

### The Date Variable
```{r, eval=FALSE}
train <- left_join(df.values, df.labels, by="id")
train <- dataprep(train)
train$date_recorded <- as.Date(as.character(train$date_recorded)) # Common date format is recognized by R

# Create "days after min date" variable
train$date_recorded <- as.integer(train$date_recorded - min(train$date_recorded))

# Filter outliers
train <- filter(train, date_recorded > 3000)

# Would normalizing the dates help at all for a random forest? Since we are making breaks and not calculating any statistics
```

```{r, eval=FALSE}
rf.1000.2 <- randomForest(as.factor(status_group) ~ ., data=train, ntree=1000)
```
```{r}
rf.1000.2
```



## Random Extra Stuff
### Generating the predictions
```{r}
test <- read.csv("test.csv")
test$status_group <- 0
test <- select(dataprep(test), -status_group)

levels(test$basin) <- levels(train$basin)
levels(test$region) <- levels(train$region)
levels(test$public_meeting) <- levels(train$public_meeting)
levels(test$scheme_management) <- levels(train$scheme_management)
levels(test$permit) <- levels(train$permit)
levels(test$extraction_type_class) <- levels(train$extraction_type_class)
levels(test$management_group) <- levels(train$management_group)
levels(test$payment) <- levels(train$payment)
levels(test$payment_type) <- levels(train$payment_type)
levels(test$water_quality) <- levels(train$water_quality)
levels(test$quantity) <- levels(train$quantity)
levels(test$source_type) <- levels(train$source_type)
levels(test$waterpoint_type) <- levels(train$waterpoint_type)
levels(test$construction_year_miss) <- levels(train$construction_year_miss)
levels(test$population_miss) <- levels(train$population_miss)

test$date_recorded <- as.Date(as.character(test$date_recorded)) # Common date format is recognized by R
# Create "days after min date" variable
test$date_recorded <- as.integer(test$date_recorded - min(test$date_recorded))


test$pred <- predict(rf.1000.2, newdata=test)

```


### K Nearest Neighbor Classification
```{r, eval=FALSE}
library(class)

# Unity based normalization
knn.train <- train
lat <- knn.train$latitude
knn.train$latitude <- (lat - min(lat))/(max(lat)-min(lat))
lon <- knn.train$longitude
knn.train$longitude <- (lon - min(lon))/(max(lon)-min(lon))
gps <- knn.train$gps_height
knn.train$gps_height <- (gps - min(gps))/(max(gps)-min(gps))

# Sample and split into training and test set
knn.train <- sample_frac(train, .8)
knn.test <- setdiff(train, knn.train)

knn.trainlabel <- knn.train[,"status_group"]
knn.trainlabel <- ifelse(as.character(knn.trainlabel)=="functional", 1, 0)
knn.train <- select(knn.train, longitude, latitude)
knn.testlabel <- knn.test[,"status_group"]
knn.testlabel <- ifelse(as.character(knn.testlabel)=="functional", 1, 0)
knn.test <- select(knn.test, longitude, latitude)

knn.pred <- knn(knn.train, knn.test, cl=knn.trainlabel, k=3)

temp <- data.frame(knn.testlabel, knn.pred)
temp <- mutate(temp, suc = ifelse(knn.testlabel==knn.pred, 1, 0))
# Clustering by longitude and latitude, 3717 7794
```



### Quantity Variable
```{r, eval=FALSE}
library(ggplot2)
ggplot(data=train) + stat_count(aes(x=status_group, fill=..count..)) + facet_grid(. ~ quantity)
```


### Correlation and Multicollinearity

Most of the variables are factors so we only need to look at a few numeric variables for multicollinearity, namely, `gps_height`, `longitude`, `latitude`, and `construction year`.

```{r}

```


### Variable Importance

We examine the importance of each variables and try to filter out unnecessary variables to simplify analysis.

(For future reference, to use more than 4gb memory we must be using 64 bit R and increase memory limit using the function memory.limit() to take advantage of a high memory machine)

```{r, eval=FALSE}
# Random Forest
library(randomForest)
fit  <-  randomForest(as.factor(status_group) ~ ., data=train, ntree=500)
varImpPlot(fit)
fit
```


We filtered our predictor variables down to 11 based on the variable importance plot.

```{r, eval=FALSE}
train <- select(train, -management_group, -public_meeting,
                       -permit, -water_quality, -basin, -amount_tsh,
                       -scheme_management, -source_type)
```

### The Date Variable
```{r, eval=FALSE}
train <- left_join(df.values, df.labels, by="id")
train$date_recorded <- as.Date(as.character(train$date_recorded)) # Common date format is recognized by R

# Create "days after min date" variable
train$date_recorded <- as.integer(train$date_recorded - min(train$date_recorded))

# Filter outliers
train <- filter(train, date_recorded > 3000)

# Would normalizing the dates help at all for a random forest? Since we are making breaks and not calculating any statistics
```










